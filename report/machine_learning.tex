\chapter{Aprendizado de Máquina}
Como \citet{bishop2006pattern} descreve, aprendizado de máquina é uma maneira de abordar um problema de computação. Nessa abordagem, a partir de um grande conjunto de dados, chamados conjunto de treinamento, são inferidos um conjunto de parâmetros a serem utilizados em um modelo parametrizado.

\section{Definições}

Algumas breves definições serão apresentadas com a finalidade de ambientar o leitor nos temas discutidos neste capítulo.

\begin{description}
\item \textbf{Conjunto de exemplos}: É o conjunto de entidades modeladas no problema. Essas entidades poderiam ser pessoas, máquinas, imagens, sinais de som, enfim qualquer objeto que se queira estudar.

\item \textbf{Características}: É um conjunto ordenado de valores que descrevem um exemplo. A escolha das características utilizadas pra representar os exemplos traz consequências diretas ao problema.

\item \textbf{Etiqueta}: É a classe que um exemplo particular pertence. Essa definição será utilizada para descrever o diversos tipos de cenário que os dados podem se encontrar.
\end{description}


\section{Categorias de Problemas}

Dentro do universo de problemas que são resolvidos pelas técnicas de aprendizado de máquina, é possível classificá-los em cinco categorias \citep{mohri2012foundations}.

\begin{description}
\item \textbf{Classificação}: Decidir a classe de um exemplo partindo das suas características, por exemplo decidir qual dígito foi escrito conhecendo uma imagem de dígito escrita a mão.
\item \textbf{Regressão}: Determinar um valor real para cada exemplo, por exemplo calcular o risco de um paciente ter contraído câncer a partir de imagens e resultados de exames.
\item \textbf{Ordenação}: Ordenar os exemplos a partir de algum critério, por exemplo listar produtos por relevância a partir das palavras chaves da busca do usuário.
\item \textbf{Agrupamento}: Particionar os exemplos em regiões homogêneas, por exemplo identificar comunidades dentro de redes sociais massivas.
\item \textbf{Redução de Dimensionalidade}: Representar o conjunto de exemplos com um número reduzido de dimensões, por exemplo comprimir imagens para processamento de imagens.
\end{description}

Neste projeto o objetivo é identificar grupos com padrão de comportamento semelhantes. A expectativa é que o comportamento dos bots formem grupos divergentes dos usuários legítimos. Todavia, nem toda máquina pertencente a esse grupo divergente está infectada, o que se quer garantir é um número de reduzido, não mais que 100, de máquinas suspeitas para serem analisadas.

\section{Cenários dos Dados}

Categoriza-se \citep{mohri2012foundations} sete cenários para os algoritmos de aprendizado, esse cenários são fortemente influenciados pelas condições dos dados de treinamento.

\begin{description}
\item \textbf{Aprendizado Supervisionado}: O modelo tem acesso a dados com os resultados de saída já esperados, ou etiquetados, como lê-se na literatura. Os problemas mais comuns desse tipo de cenário são classificação, regressão e ordenação.

\item \textbf{Aprendizado Não Supervisionado}: Só se dispõe da conjunto de exemplos para treinamento sem etiquetas. Geralmente é mais utilizado para classificação, agrupamento e redução de dimensionalidade

\item \textbf{Aprendizado Semi-Supervisionado}: Neste cenário, é possível acessar uma conjunto de exemplos sem etiquetas e uma com etiquetas. Esse é o caso de problemas em que dados sem etiquetas são fáceis de serem adquiridos, ao contrário dos dados etiquetados, pela dificuldade de etiquetar.

\end{description}

E ainda há outros possíveis cenários ainda mais complexos e específicos. Esses cenários não foram catalogados neste trabalho porque a campo de Aprendizado de Máquina está ainda em constante fase de crescimento.

No caso deste trabalho, algumas máquinas identificadas pelo IP foram confirmadas como bots, mas não há garantia de que sejam os únicos bots na rede. O algoritmo se vê num cenário não supervisionado, mas os desenvolvedores validam os resultados a partir dos bots já conhecidos.

Note que se as máquinas conhecidamente suspeitas não se encontrarem no grupo divergente, o sistema falhou.

\section{K-Médias}

Segundo \citet{witten2011data}, K-Médias é um algoritmo de aprendizado iterativo baseado na distância geométrica. Inicialmente o algoritmo inicializa \textit{K} centroides em posições distintas. Cada exemplo, então, é associado ao centroide mais próximo. Em seguida o centroide atualiza sua posição para a média das posições dos pontos associados a ele. A partir de então recomeça o ciclo, são reassociados os pontos que estão mais próximos do centroide em sua nova posição.

Note que a função custo que se espera minimizar é a soma de todas as distâncias entre os exemplos e seus respectivos centroides, como descrito na seguinte equação

\[
\sum_{i=1}^{n} \lVert cent(\mathbf{X_{i}}) - \mathbf{X_{i}} \rVert
\]

O qual \(cent(\mathbf{X})\) é a função que retorna o centroide mais próximo das coordenadas \(\mathbf{X}\)

\begin{figure}
\includegraphics[width=\textwidth]{k_means_example}
\caption[Aplicação do K-Médias]{Aplicação do K-Médias} \label{fig:k_means_example}
\end{figure}

A \ref{fig:k_means_example} representa a aplicação da técnica em dados bidimensionais com três grupos. É possível notar que converge rapidamente, nesse caso em apenas 9 iterações chegou-se a um resultado satisfatório. Além disso, espera-se uma quantidade de máquinas da ordem de \(10^4\), como o algoritmo K-Médias é linear tanto na decisão do grupo para cada instância quanto no cálculo do novo centroide, a temporização do algoritmo é muito baixa.

\subsection{Determinação do Número de Grupos}

Para a operação plena do algoritmo é necessário que seja informada a quantidade de grupos que são buscados. Isso pode ser um problema simples quando é possível visualizar os dados, como na figura \ref{fig:k_means_example}, mas não é possível realizar o mesmo procedimento quando os exemplos têm mais que três dimensões, como no caso deste projeto.

\begin{figure}
\includegraphics[width=\textwidth]{between_two_clusters}
\caption[Centróide Pouco Representativo]{Centróide Pouco Representativo} \label{fig:between_two_clusters}
\end{figure}

Além disso, se por um lado, pode-se carecer centroides levando a cenários como na figura \ref{fig:between_two_clusters}, no qual o centroide não generaliza os dados, por outro, como é possível visualizar na figura \ref{fig:cluster_overfitting}, um grupo pode acabar se separando por centroides desnecessários.

\begin{figure}[htbp]
\centering
\includegraphics[scale=0.5]{cluster_overfitting}
\caption[Grupo Dividido]{Grupo Dividido} \label{fig:cluster_overfitting}
\end{figure}

Para resolver esse problema utiliza-se o Método \textit{Elbow} \citep{kodinariya2013review}. Trata-se de um método visual no qual procura-se o ponto em que a adição de um novo centroide não implica mais na redução drástica da função custo. Esse efeito é observado na figura \ref{fig:elbow}

\begin{figure}[htbp]
\centering
\includegraphics[scale=0.5]{elbow}
\caption[Ponto Crítico da Otimização]{Ponto Crítico da Otimização} \label{fig:elbow}
\end{figure}

O artigo \citep{kodinariya2013review} também cita outros métodos para com critérios matemáticos menos subjetivos, porém este foi utilizado para fins de simplicidade. No capítulo 5 relativo aos experimentos, esse método será avaliado para os dados que utilizados neste trabalho.